---
title: "Coronary Data (BBN vs. GLM)"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
if (!require("pacman")) {
  install.packages("pacman")
  library(pacman)
}

pacman::p_load(tidyverse)
pacman::p_load(bnlearn)
pacman::p_load(ggplot2)
pacman::p_load(caret)
pacman::p_load(kable)
pacman::p_load(kableExtra)
pacman::p_load(MASS)
pacman::p_load(e1071)

knitr::opts_chunk$set(echo = FALSE)
```

### Purpose

The objective of this document is to build a Bayesian Belief Network (BBN), and compare it to a logistic GLM for classification tasks.  I'm mostly looking for exposure to the library for a task that actually justifies it's use (unlike the data set explored in this document).  The data set of interest is the "coronary" data, which is included with the *bnlearn* package.  

### Dataset

We'll use the coronary data set to try to predict High Blood Pressure ("Pressure").  The variables are shown below with their levels.  High Blood Pressure corresponds to instances of the "Pressure" variable with level ">140".

```{r dataset-overview}

data("coronary")
coronary_df <- data.frame(coronary)
str(coronary_df)

```

### Class balance

GLMs can be sensitive to extreme class-imbalance.  We observe a slight imbalance in the instance counts; we'll just ignore it.

```{r dataset-class-balance}

table(coronary_df$Pressure) %>%
  kable(col.names=c("Pressure Level", "Count"), caption="Pressure Variable Counts") %>%
  kable_styling()

```


### Predictor Identification

Let's examine each feature in relation to the variable of interest to see which features might be good predictors.  The tables below show the percentage of variable levels within each class (High or Low Blood Pressure).  Ideally, we'll see some variables that are strongly correlated with "Pressure". 

```{r dataset-vis, results='asis'}

table_captions = list(
  Smoking="Tobacco Use",
  M..Work="Mentally Strenuous Work",
  P..Work="Physically Strenuos Work",
  Proteins="Ratio of Alpha to Beta Lipoproteins",
  Family="Family history of Heart Disease"
)

for (feature_name in names(coronary_df)) {
  if (feature_name != "Pressure") {
    table(coronary_df$Pressure, coronary_df[[feature_name]]) %>%
      prop.table(margin=1) %>%
      `*`(100) %>%
      round(2) %>%
      `rownames<-`(c("Low Blood Pressure (<140)", "High Blood Pressure (>140)"))%>%
      kable(caption=sprintf("%s (%s)", table_captions[[feature_name]], feature_name)) %>%
      kable_styling() %>%
      print()
  }
}
```

Unfortunately, nothing really jumps out too much.  Mentally strenuous work perhaps shows a small effect, as do Protein Levels.  Surprisingly, Tobacco use didn't seem to have much of an effect.

### Baseline Model (GLM)

```{r model-data-split}

set.seed(1234)

testIdx <- as.integer(
  createDataPartition(coronary_df$Pressure, p=0.2)$Resample1
)

coronary_df.train <- coronary_df[-testIdx, ]
coronary_df.test <- coronary_df[testIdx, ]

rm(testIdx)
```

We'll start with a logistic regression as the baseline model.  We'll use a step-wise logistic GLM to select the model with the lowest AIC. 

```{r model-logistic, message=FALSE, echo=TRUE}

lm.fitControl <- trainControl(
  method="cv",
  number=7
)

# find a logistic regression model using AIC for feature selection
lm.train <- train(Pressure ~ ., data=coronary_df.train,
                  method="glmStepAIC", family=binomial(link='logit'),
                  trControl=lm.fitControl, trace=FALSE)

# display the model (with the features selected)
lm.fit <- lm.train$finalModel
print(lm.fit)

# show the confusion matrix
lm.cfm <- confusionMatrix(lm.train)
print(lm.cfm)
```

Although the accuracy is better than 50-50, we only detect the event of interest (HBP) **`r round(lm.cfm$table[2,2] / sum(lm.cfm$table[,2]), 2)`%** of the time.  Let's see if we can do better with a Bayesian Belief Network (BBN).

### Bayesian Belief Network

We'll see if a BBN does any better.  First we need to specify or induce a graph for the network.  We'll give the Hill-Climbing (*hc*) algorithm some guidance; from our initial analysis we have reason to suspect that Proteins and M..Work have a significant impact on Pressure.

```{r model-bbn-structure, echo=TRUE}

bbn.structure <- hc(coronary_df,
                    whitelist=data.frame(from=c("Proteins", "M..Work"), to=c("Pressure", "Pressure")))

plot(bbn.structure)

```

We'll need to remove one edge from the network.  It doesn't make any sense that M..Work would influence family history, so we'll remove it.  Also, there was little evidence in our initial analysis that it directly influences Pressure.

To make a judgement about what to do with the P..Work variable, we'll first examine it's relationship with smoking.

```{r model-bnn-pwork-vs-smoke, results='asis', echo=FALSE}

table(coronary_df$P..Work, coronary_df$Smoking) %>%
  prop.table(margin=1) %>%
  `*`(100) %>%
  round(2) %>%
  kable(caption="Physically Strenuous work (rows) vs. Tobacco Use (cols)") %>%
  kable_styling() 


```

Looking at the diagonals gives us little reason to suspect correlation, so we'll remove it as well (although we could just be lazy and leave it alone, since it isn't impacting our Pressure prediction).  The final network structure is shown below.  This model should perform no differently than *Naive Bayes*, however the point of this exercise is to get acquainted with bnlearn.

```{r model-bbn-structure2}

# remove the connection between M..work and Family
bbn.replaceidx <-  with(data.frame(bbn.structure$arcs), {
  which(to == "Family" & from == "M..Work")})
bbn.structure$arcs <- bbn.structure$arcs[-bbn.replaceidx, ]

# remove connection between P..Work and Smoking
bbn.replaceidx <-  with(data.frame(bbn.structure$arcs), {
  which(to == "P..Work" & from == "Smoking")})
bbn.structure$arcs <- bbn.structure$arcs[-bbn.replaceidx, ]

# remove connection between P..Work and M..Work
bbn.replaceidx <-  with(data.frame(bbn.structure$arcs), {
  which(to == "P..Work" & from == "M..Work")})
bbn.structure$arcs <- bbn.structure$arcs[-bbn.replaceidx, ]

# remove connection between N..Work and Pressure
bbn.replaceidx <-  with(data.frame(bbn.structure$arcs), {
  which(to == "Pressure" & from == "M..Work")})
bbn.structure$arcs <- bbn.structure$arcs[-bbn.replaceidx, ]

# add connection for family and pressure
bbn.structure$arcs <- rbind(bbn.structure$arcs, c("Family", "Pressure"))

plot(bbn.structure)

```

```{r bbn-model-train}

bbn.fit <- bn.fit(bbn.structure, data=coronary_df.train, method="mle")

# check the residuals
bbn.fit.resid <- predict(bbn.fit, "Pressure", coronary_df.train)

print("Confusion Matrix")
confusionMatrix(bbn.fit.resid, coronary_df.train$Pressure)

```

There is a negligible difference in accuracy between the models; let's move on to final results.

### Test Set (final results)

```{r model-run-test}

lm.fit.test <- predict(lm.train, coronary_df.test)
bbn.fit.test <-  predict(bbn.fit, "Pressure", coronary_df.test)

lm.test.cm <- confusionMatrix(lm.fit.test, coronary_df.test$Pressure)
bbn.test.cm <- confusionMatrix(bbn.fit.test, coronary_df.test$Pressure)

```


The below confusion matrices are shown as percentages.  In general, the models do no better than random chance.  The difference in predictive power is probably explainable by the lack of k-fold cross validation on the bayesian model.

* GLM: `r 100*round(lm.test.cm$overall[["Accuracy"]],2)`%
* BBN: `r 100*round(bbn.test.cm$overall[["Accuracy"]],2)`%

```{r model-compare}

lm.test.cm$table %>%
  prop.table() %>%
  `*`(100) %>%
  round(2) %>%
  kable(caption="GLM Confusion Matrix, actual (cols) vs pred (rows)") %>%
  kable_styling()

bbn.test.cm$table %>%
  prop.table() %>%
  `*`(100) %>%
  round(2) %>%
  kable(caption="BBN Confusion Matrix, actual (cols) vs pred (rows)") %>%
  kable_styling()


```



