---
title: "Coronary Data (BBN vs. GLM)"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
if (!require("pacman")) {
  install.packages("pacman")
  library(pacman)
}

pacman::p_load(tidyverse)
pacman::p_load(bnlearn)
pacman::p_load(ggplot2)
pacman::p_load(caret)
pacman::p_load(kable)
pacman::p_load(kableExtra)
pacman::p_load(MASS)
pacman::p_load(e1071)

knitr::opts_chunk$set(echo = FALSE)
```

### Purpose

The objective of this document is to build a Bayesian Belief Network (BBN), and compare it to a logistic GLM for classification tasks.  I'm mostly looking for exposure to the library for a task that actually justifies it's use (unlike the data set explored in this document).  The data set of interest is the "coronary" data, which is included with the *bnlearn* package.  

### Dataset

We'll use the coronary data set to try to predict High Blood Pressure ("Pressure").  The variables are shown below with their levels.  High Blood Pressure corresponds to instances of the "Pressure" variable with level ">140".

```{r dataset-overview}

data("coronary")
coronary_df <- data.frame(coronary)
str(coronary_df)

```

### Class balance

GLMs can be sensitive to extreme class-imbalance.  We observe a slight imbalance in the instance counts; we'll just ignore it.

```{r dataset-class-balance}

table(coronary_df$Pressure) %>%
  kable(col.names=c("Pressure Level", "Count"), caption="Pressure Variable Counts") %>%
  kable_styling()

```


### Predictor Identification

Let's examine each feature in relation to the variable of interest to see which features might be good predictors.  The tables below show the percentage of variable levels within each class (High or Low Blood Pressure).  Ideally, we'll see some variables that are strongly correlated with "Pressure". 

```{r dataset-vis, results='asis'}

table_captions = list(
  Smoking="Tobacco Use",
  M..Work="Mentally Strenuous Work",
  P..Work="Physically Strenuos Work",
  Proteins="Ratio of Alpha to Beta Lipoproteins",
  Family="Family history of Heart Disease"
)

for (feature_name in names(coronary_df)) {
  if (feature_name != "Pressure") {
    table(coronary_df$Pressure, coronary_df[[feature_name]]) %>%
      prop.table(margin=1) %>%
      `*`(100) %>%
      round(2) %>%
      `rownames<-`(c("Low Blood Pressure (<140)", "High Blood Pressure (>140)"))%>%
      kable(caption=sprintf("%s (%s)", table_captions[[feature_name]], feature_name)) %>%
      kable_styling() %>%
      print()
  }
}
```

Unfortunately, nothing really jumps out too much.  Mentally strenuous work perhaps shows a small effect, as do Protein Levels.  Surprisingly, Tobacco use didn't seem to have much of an effect.

### Baseline Model (GLM)

```{r model-data-split}

set.seed(1234)

testIdx <- as.integer(
  createDataPartition(coronary_df$Pressure, p=0.2)$Resample1
)

coronary_df.train <- coronary_df[-testIdx, ]
coronary_df.test <- coronary_df[testIdx, ]

rm(testIdx)
```

We'll start with a logistic regression as the baseline model.  We'll use a step-wise logistic GLM to select the model with the lowest AIC. 

```{r model-logistic, message=FALSE, echo=TRUE}

lm.fitControl <- trainControl(
  method="cv",
  number=7
)

# find a logistic regression model using AIC for feature selection
lm.train <- train(Pressure ~ ., data=coronary_df.train,
                  method="glmStepAIC", family=binomial(link='logit'),
                  trControl=lm.fitControl, trace=FALSE)

# display the model (with the features selected)
lm.fit <- lm.train$finalModel
print(lm.fit)

# show the confusion matrix
lm.cfm <- confusionMatrix(lm.train)
print(lm.cfm)
```

Although the accuracy is better than 50-50, we only detect the event of interest (HBP) **`r round(lm.cfm$table[2,2] / sum(lm.cfm$table[,2]), 2)`%** of the time.  Let's see if we can do better with a Bayesian Belief Network (BBN).

### Bayesian Belief Network

We'll see if a BBN does any better.  First we need to specify or induce a graph for the network.  We'll start with an empty graph, and we're going to just configure the network to mimic Naive Bayes.

```{r model-bbn-structure}

bbn.structure <- model2network("[Smoking][Family][P..Work][M..Work][Proteins][Pressure|Smoking:Family:P..Work:M..Work:Proteins]")
plot(bbn.structure)

```

```{r bbn-model-train}

bbn.fit <- bn.fit(bbn.structure, data=coronary_df.train, method="mle")

# check the residuals
bbn.fit.resid <- predict(bbn.fit, "Pressure", coronary_df.train)

print("Confusion Matrix")
confusionMatrix(bbn.fit.resid, coronary_df.train$Pressure)

```

There is a negligible difference in accuracy between the models; let's move on to final results.

### Test Set (final results)

```{r model-run-test}

lm.fit.test <- predict(lm.train, coronary_df.test)
bbn.fit.test <-  predict(bbn.fit, "Pressure", coronary_df.test)

lm.test.cm <- confusionMatrix(lm.fit.test, coronary_df.test$Pressure)
bbn.test.cm <- confusionMatrix(bbn.fit.test, coronary_df.test$Pressure)

```


The below confusion matrices are shown as percentages.  In general, the models do no better than random chance.  The difference in predictive power is probably explainable by the lack of k-fold cross validation on the Bayesian model.

* GLM: `r 100*round(lm.test.cm$overall[["Accuracy"]],2)`%
* BBN: `r 100*round(bbn.test.cm$overall[["Accuracy"]],2)`%

```{r model-compare}

lm.test.cm$table %>%
  prop.table() %>%
  `*`(100) %>%
  round(2) %>%
  kable(caption="GLM Confusion Matrix, actual (cols) vs pred (rows)") %>%
  kable_styling()

bbn.test.cm$table %>%
  prop.table() %>%
  `*`(100) %>%
  round(2) %>%
  kable(caption="BBN Confusion Matrix, actual (cols) vs pred (rows)") %>%
  kable_styling()


```

### Final Conclusions

Given the large data set size, and the low dimensional (there are a maximum 32 (2^5) unique combinations in the data set), my hunch is that there is no sufficient "decision boundary" in the data, and Naive Bayes really is the best model.  This should be easy to verify by looking at the results of the model, and seeing if it aligns with the most common value in the training dataset.  In other words, we've identified the "best" fit, and it's a Naive Bayes (which we've replicated with the BNN).

** Note: there are a handful of combinations that are a 50-50 split, which sporadically cause "NO" to show up in a handful of places.**

```{r results-check}

mode <- function(fctr){
  fctr_levels <- levels(fctr)
  fctr_levels[which.max(tabulate(fctr))]
}

check_data <- coronary_df.train
check_data.predictions <- predict(bbn.fit, "Pressure", check_data, prob=TRUE)
check_data <- cbind(check_data,  data.frame(PredictedPressure=check_data.predictions, PredictedProb=as.vector(attr(check_data.predictions, "prob"))))

check_data %>% 
  dplyr::select(Smoking, M..Work, P..Work, Proteins, Family, ActualPressure=Pressure, PredictedPressure) %>%
  dplyr::group_by(Smoking, M..Work, P..Work, Proteins, Family) %>%
  summarize(TrueMode=mode(ActualPressure), PredMode=mode(PredictedPressure)) %>%
  mutate(`Same?` = if_else(TrueMode==PredMode, "YES", "NO")) %>%
  kable() %>%
  kable_styling()

```






