---
title: "Iris Data"
author: "Mike McPhee Anderson"
date: "December 21, 2018"
output: html_document
---

```{r setup, include=FALSE}

if (!require(pacman)) {
  install.packages("pacman")
  library("pacman")
}

pacman::p_load(tidyverse, datasets, kableExtra, 
               caret, rpart, rpart.plot, e1071,
               nnet, NeuralNetTools)

data(iris)

set.seed(142)

```

## Iris Data Set

The iris (flower) dataset is commonly used for classification problems.  The objective is to identify the iris's *species* based upon measurements of the *sepal* and *petal*.  Summary of the dataset is below:

```{r data-summary, echo=FALSE}

options(width=500)

summary(iris)

```

## Visualization

### Basic Clustering

Start by clustering based on size.  Both the Sepal and Petal have length and width; perform a dimensionality reduction by converting this to be a size.  Two new attributes will be added, "Sepal.Size" and "Petal.Size".  

```{r clustering-by-size, echo=FALSE}

# Add the new attributes
iris <- iris %>% mutate(
  Sepal.Size = Sepal.Length * Sepal.Width,
  Petal.Size = Petal.Length * Petal.Width
)

# Plot it
ggplot(iris, aes(x=Sepal.Size, y=Petal.Size, col=Species, shape=Species)) + geom_point()

```


That worked pretty well, it's apparent that the *setosa* has a small petal size, and *virginica* has a large petal size.  We'll add preliminary lines to mimic the rules that a decision tree might find.  It appears that Petal size is a good predictor


```{r}

# Plot it with an "eye-balled" decision boundary on Petal.Size
ggplot(iris, aes(x=Sepal.Size, y=Petal.Size, col=Species, shape=Species)) + 
  geom_point() + 
  geom_hline(yintercept = 8) +
  geom_hline(yintercept = 2.5)


```


## Classification Models

### Decision Tree

With 150 observations in the data set, 5-folds sounds about right for *k-fold cross-validation*.  We'll fit an actual decision tree to the data and plot the results.  We'll perform training on a subset and validating against a final hold-out set. 

```{r echo=TRUE}

# create holdout set
trainIdx <- as.vector(
  createDataPartition(iris$Species, p=.80, list=FALSE))

iris.train <- iris[trainIdx,]
iris.test <- iris[-trainIdx,]

# four folds
trCtrl <- trainControl(method="cv", number=4)

# tuneLength is the number of attempts made to adjust the model's "tuning" parameter,
# in this case it is the "complexity" of the decision tree, e.g. gini information gain
treeFit <- train(Species ~ Petal.Size, data=iris.train, 
                 method="rpart",
                 trControl=trCtrl,
                 tuneLength=10)

# plot the tree
prp(treeFit$finalModel)

# final error rate
iris.test$predict <- predict(treeFit$finalModel, iris.test, type="class")

dtreeErrorRate <- 1 - sum(iris.test$predict == iris.test$Species)/nrow(iris.test)

```

### Decision Tree results
The final error rate on the validation set is `r round(dtreeErrorRate*100,2)`%.

### Neural Network
Neural network inputs typically operate on input values between 0 and 1.  Transform the length and width measurements to be normalized, i.e. "scale-free".  

```{r transform-for-nnet}

## function to "normalize" attributes, they will range between 0-1
descale <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

# create normalized versions of the attributes
iris$Sepal.Length.Reg <- descale(iris$Sepal.Length)
iris$Sepal.Width.Reg <- descale(iris$Sepal.Width)
iris$Petal.Length.Reg <- descale(iris$Petal.Length)
iris$Petal.Width.Reg <- descale(iris$Petal.Width)

# resample
iris.train <- iris[trainIdx,]
iris.test <- iris[-trainIdx,]

```

The effectiveness of a neural-network is dependent on the selection of an appropriate structure.  Utilize the *caret* package to determine an appropriate number of hidden layers.  As a rule of thumb, the number of hidden layers in the network shouldn't exceed the formula below:

$$ N^{hidden} = \frac{N^{observations}}{\alpha \times (N^{input} + N^{ouput})} $$

```{r include=FALSE}

## determine the maximum number of hidden layer neurons
maxHiddenLayerNeurons <- ceiling(nrow(iris) / (2 * (4 + 3)))

```

Alpha is arbitrary, and will be set equal to two.  The maximum number of hidden layers is `r maxHiddenLayerNeurons`.  The caret package will do a search to find an appropriate number of hidden layer neurons.

```{r caret-train-nnet}

trCtrl <- trainControl(method="repeatedcv", number=4, repeats=3)

trGrid <- expand.grid(size=1:maxHiddenLayerNeurons, decay=c(0.005))

# fit network, capture.output suppresses annoying output messages
discardOutput <- capture.output(
  nnFit <- train(Species ~ Sepal.Length.Reg + Sepal.Width.Reg + 
                 Petal.Length.Reg + Petal.Width.Reg, data=iris.train,
               method="nnet", trControl=trCtrl, tuneLength=10,
               tuneGrid=trGrid))

```

The results of the tuning the neural network is shown below.  Using the maximum number of hidden layers provides no better results.  The decay parameter is for regularization and serves as a pentalty function to avoid over-fitting the model.  It looks like 2 - 5 neurons in the hidden layer provide a good fit.  We'll use 3. 

```{r}
plot(nnFit)
```

```{r retrain-final-nn}

# re-train with 4 hidden layer neurons
irisNN <- nnet(Species ~ Sepal.Length.Reg + Sepal.Width.Reg + 
                 Petal.Length.Reg + Petal.Width.Reg, size=3,
                 data=iris.train, decay=0.005, trace=FALSE)

plotnet(irisNN)

```

Let's apply the model to the testing data, and calculate the most probable class and error rate.

```{r test-final-nn}

iris.test$predictNN <- predict(irisNN, iris.test, type="class")

nnErrorRate <- 1 - sum(iris.test$predictNN == iris.test$Species)/nrow(iris.test)

```

The final error rate is `r round(nnErrorRate * 100, 2)`%.



