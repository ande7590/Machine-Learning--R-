---
title: "Wine"
author: "Mike McPhee Anderson"
date: "December 24, 2018"
output: html_document
---

```{r setup, include=FALSE}

if (!require(pacman)) {
  install.packages("pacman")
  library("pacman")
}

pacman::p_load(tidyverse, datasets, kableExtra, 
               caret, rpart, rpart.plot, e1071,
               nnet, NeuralNetTools, GGally, gridExtra,
               kernlab)

wine <- read.csv("data/wine.csv")
wine$class <- as.factor(wine$class)

set.seed(47)
```

## Wine

The wine data is a high-dimensional dataset with 3 classes for prediction.  The objective is to predict the class (i.e. type of wine, labeled 1 through 3) based on 13 continuous attributes representing measurements of various chemical properties of the wine.

## Visualization

Since the data is highly dimensional, visualization will be difficult.  We'll throw the kitchen sink at it and do pair-plots.

```{r vis-pair-plots, warning=FALSE, message=FALSE, fig.width=12, fig.height=10}

#ggpairs(wine, aes(color=class, alpha=0.65))

```

### Basic Visualization

Looking at the above plot, *alcohol* and *od* seem to form a decent decision boundary, so does *flavanoid* and *proline*.  We'll eye-ball some boundaries and add them to the graph (black lines).

```{r vis-drill-1, fig.width=10, fig.height=3}
plot.alcohol.vs.od <- ggplot(wine, aes(x=alcohol, y=od, color=class)) +
    geom_point() +
    geom_hline(yintercept = 2.1) +
    geom_vline(xintercept = 12.8)

plot.flav.vs.proline <- ggplot(wine, aes(x=flavanoids, y=proline, color=class)) +
    geom_point() +
    geom_abline(intercept = 1580, slope = -(1500/4.5)) +
    geom_abline(intercept = 120, slope = (1500/4.5))

grid.arrange(plot.alcohol.vs.od, plot.flav.vs.proline, ncol=2)
```

The first plot (od vs. alcohol) looks to form a pretty straight-forward decision boundary, but the second would be a struggle with a decision-tree.  It seems that a rotation could fix it, since the decision boundary is on a line.  We'll perform an SVD to (hopefully) remove this rotation, and plot the resulting points.


```{r vis-svd}

# rotate the proline / flavanoid plot to make a cleaner decision boundary (horizontal / vertical lines)
plot2Rot <- svd(cbind(wine$proline, wine$flavanoids))
plot2Translated <- as.data.frame(cbind(plot2Rot$u, wine$class))
names(plot2Translated) <- c("proline", "flavanoids", "class")

plot2Translated$class <- as.factor(plot2Translated$class)

ggplot(plot2Translated, aes(x=flavanoids, y=proline, color=class)) + 
  geom_point() +
  geom_hline(yintercept = -0.066) +
  geom_vline(xintercept = 0.02) + 
  geom_hline(yintercept = -0.09)


```

Looks like we got some pretty good sepearation, although not a clean as we probably could get with a manual rotation / transformation.  Adding another region seems to fix it though, so this might still be a good candidate for a decision tree.

### Advanced Visualization

The SVD factorization is applied during PCA.  It might be interesting to visualize the  pricipal components.

```{r vis-do-pca}

wine.M <- as.matrix(wine[ , 2:14])
wine.prComp <- prcomp(wine.M)

summary(wine.prComp)

```

Judging by the cumulative proportion... that worked pretty well.  Let's take a look at a pair plot of the principal components.

```{r vis-pairs-pca, warning=FALSE, message=FALSE, fig.width=12, fig.height=10}

wine.pca <- as.data.frame(cbind(wine.prComp$x, class=wine$class))

wine.pca$class = as.factor(wine.pca$class)

#ggpairs(wine.pca, aes(color=class))

```

It looks like there might be a couple candidates for good decision boundaries.  The first principal component does an OK job separating red from the others, and the fourth does a good job of separating green from blue.  Looking at them together looks decent, but not great.

```{r vis-pca-drill}

ggplot(wine.pca, aes(x=PC1, y=PC4, color=class)) +
  geom_point()

```


## Feature Selection

Although some feature look promising from visualization, it might be a good idea to try some automated feature selection too. First we should get a feel for which features (if any) are highly correlated. 

```{r features-corr}

cor(wine[,2:ncol(wine)])

```

*od* is pretty correlated with *total.phenols*, *flavanoids*, and *hue*.  *proanthocyanins* stands out as having a few highly correlated other variables as well.  This suggests that we should use the "big-guns" to help with feature selection.  The caret pacakge has some functionality to help with this.

```{r features-lvq}

trCtrl <- trainControl(method="repeatedcv", number=5, repeats=3)
lvqFit <- train(class~., data=wine, method="lvq", preProcess="scale", trControl=trCtrl)

lvqFitImportance <- varImp(lvqFit, scale=FALSE)

print(lvqFitImportance)

plot(lvqFitImportance)

```

The results suggest elimninating a couple variables.  Next, we'll check the PCA as well. 

```{r features-lvq-pca}

trCtrl <- trainControl(method="repeatedcv", number=5, repeats=3)
lvqFit <- train(class~., data=wine.pca, method="lvq", trControl=trCtrl)

lvqFitImportance <- varImp(lvqFit, scale=FALSE)

print(lvqFitImportance)

plot(lvqFitImportance)

```

PCA did it's job, we can probably get away with using about half the principal componenets.  Consistent with our analysis above, the first and fourth principal components look pretty important.  Components 3, 5, and 6 look pretty important too.

## Models

### Decision Tree

We'll holdout 20% of the training set for testing (stratified).  

```{r split-train-test}

wine$svd.flavanoids = plot2Translated$flavanoids
wine$svd.proline = plot2Translated$proline
wine <- cbind(wine, wine.pca[, 1:13])

# create holdout set
trainIdx <- as.vector(
  createDataPartition(wine$class, p=.80, list=FALSE))

wine.train <- wine[trainIdx,]
wine.test <- wine[-trainIdx,]

```

First we'll attempt a decision tree model on *od*, *alcohol*, *flavanoid*, and *proline*.  We'll do this using the non-transformed (via SVD) data first.

```{r train-decision-tree-nontransformed}
trCtrl <- trainControl(method="repeatedcv", number=8, repeats = 3)

# tuneLength is the number of attempts made to adjust the model's "tuning" parameter,
# in this case it is the "complexity" of the decision tree, e.g. gini information gain
treeFit <- train(class ~ od + alcohol + flavanoids + proline, data=wine.train, 
                 method="rpart",
                 trControl=trCtrl,
                 tuneLength=10)

confusionMatrix(treeFit)

```

The results are not great, let's try with the svd translated flavanoids and svd.


```{r train-decisiontree-transformed}
trCtrl <- trainControl(method="repeatedcv", number=8, repeats = 3)

# tuneLength is the number of attempts made to adjust the model's "tuning" parameter,
# in this case it is the "complexity" of the decision tree, e.g. gini information gain
treeFit <- train(class ~ od + alcohol + svd.flavanoids + svd.proline, data=wine.train, 
                 method="rpart",
                 trControl=trCtrl,
                 tuneLength=10)

confusionMatrix(treeFit)

```

Suprisingly, this has little effect on accuracy. Let's re-attempt this with SVM.  This should function better than the decision tree, since the boundary doesn't need to be a straight line (as with the decision tree models above).

### Support Vector Machine

```{r train-svm-basic}
trCtrl <- trainControl(method="repeatedcv", number=8, repeats = 5)

trGrid <- expand.grid(C=seq(0.05, 1.5, .1))

# tuneLength is the number of attempts made to adjust the model's "tuning" parameter,
# in this case it is the "complexity" of the decision tree, e.g. gini information gain
svmFit1 <- train(class ~ od + alcohol + flavanoids + proline, data=wine.train, 
                 preProcess="scale",
                 method="svmLinear",
                 trControl=trCtrl,
                 tuneGrid=trGrid,
                 tuneLength=10)

confusionMatrix(svmFit1)

```

Pretty decent improvement, let's see how if applying svm to the principal components improves things.  This is pretty close to applying SVD.

```{r train-svm-pca}
trCtrl <- trainControl(method="repeatedcv", number=8, repeats = 5)

trGrid <- expand.grid(C=seq(0.05, 1.5, .1))

# tuneLength is the number of attempts made to adjust the model's "tuning" parameter,
# in this case it is the "complexity" of the decision tree, e.g. gini information gain
svmFit2 <- train(class ~ PC1 + PC4 + PC3 + PC5 + PC6, data=wine.train,
                 method="svmLinear",
                 trControl=trCtrl,
                 tuneGrid=trGrid,
                 tuneLength=10)

confusionMatrix(svmFit2)

```

There is a small improvement to accuracy; this might be someone spurios due to additional parameter (i.e. degree of freedom).

### Random Forest

TODO: but there is little reason to believe this will perform better than SVM without some feature engineering on the underlying data.  The next step would be trying to tease apart some of the class 2 / 3 instances that are being misclassified.

## Final Results

```{r final-results}

finalTestResults1 <- predict(svmFit1$finalModel, wine.test %>% select(od, alcohol, flavanoids, proline))
finalTestResults2 <- predict(svmFit2$finalModel, wine.test %>% select(PC1, PC4, PC3, PC5, PC6))

errorRate1 <- 1 - sum(finalTestResults1 == wine.test$class) / length(finalTestResults1)
errorRate2 <- 1 - sum(finalTestResults2 == wine.test$class) / length(finalTestResults2)

```

So far, the best model for this problem is the support vector machine on the principal components.  The final errorRate for the first SVM model (no PCA) is `r round(errorRate1*100, 1)`%.  The error rate for the second SVM model (with PCA) is `r round(errorRate2*100, 1)`%.  The second model clealy outperforms the first.


